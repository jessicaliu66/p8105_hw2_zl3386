p8105_hw2_zl3386
================
Ziqiu Liu
2023-09-26

``` r
library(tidyverse)
library(readxl)
```

## Problem 1

This problem uses the *FiveThirtyEight* data.

In particular, we’ll use the data in `pols-month.csv`,
`unemployment.csv`, and `snp.csv`.

Our goal is to merge these into a single data frame using `year` and
`month` as keys across datasets.

**First**, clean the data in `pols-month.csv`.

- Use `separate()` to break up the variable `mon` into integer variables
  `year`, `month`, and `day`
- Replace month number with month name
- Create a `president` variable taking values `gop` and `dem`, and
  remove `prez_dem` and `prez_gop`
- Remove the `day` variable.

``` r
pols_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "/", convert = TRUE) |>
  mutate(
    president = case_match(
      prez_dem,
      1 ~ "dem",
      2 ~ "dem",
      0 ~ "gop"
    ),
    month = case_match(
      month,
      1 ~ "Jan",
      2 ~ "Feb",
      3 ~ "Mar",
      4 ~ "Apr",
      5 ~ "May",
      6 ~ "Jun",
      7 ~ "Jul",
      8 ~ "Aug",
      9 ~ "Sep",
      10 ~ "Oct",
      11 ~ "Nov",
      12 ~ "Dec"
    )
  ) |>
  select(-starts_with("prez_"), -day)
```

**Second**, clean the data in `snp.csv` using a similar process to the
above.

For consistency across datasets, arrange according to `year` and
`month`, and organize so that `year` and `month` are the leading
columns.

``` r
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() 

snp_df_1 = 
  snp_df[1:175,] |>
  janitor::clean_names() |>
  separate(date, into = c("year", "month", "day"), sep = "/", convert = TRUE) 

snp_df_2 = 
  snp_df[176:787,] |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day" ,"year"), sep = "/", convert = TRUE) |>
  relocate(year, month) |>
  mutate(
    year = year + 1900,
    year = replace(year, year == 1900, 2000)
  )

snp_df = 
  rbind(snp_df_1, snp_df_2) |>
  mutate(
    month = case_match(
      month,
      1 ~ "Jan",
      2 ~ "Feb",
      3 ~ "Mar",
      4 ~ "Apr",
      5 ~ "May",
      6 ~ "Jun",
      7 ~ "Jul",
      8 ~ "Aug",
      9 ~ "Sep",
      10 ~ "Oct",
      11 ~ "Nov",
      12 ~ "Dec"
    )
  ) |>
  select(-day) |>
  arrange(year, month)
```

**Third**, tidy the `unemployment` data so that it can be merged with
the previous datasets.

This process will involve switching from “wide” to “long” format;
ensuring that key variables have the same name; and ensuring that key
variables take the same values.

``` r
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |>
  janitor::clean_names() 
```

Join the datasets by merging `snp` into `pols`, and merging
`unemployment` into the result.

``` r
results_df =
  full_join(snp_df, pols_df, by = c("year", "month")) |>
  full_join(unemployment_df, by = c("year", "month"))
```

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

- Dataset `pols_df` contains the date (`year` and `month`) , the party
  of the president (`president`) and the number of national politicians
  who are democratic or republican on the given date.

- Dataset `snp_df` contains 3 columns which indicate the date (`year`
  and `month`) and the closing values of the S&P stock index on the
  associated date (`close`).

- Dataset `unemployment_df` contains 3 columns which indicate the date
  (`year` and `month`) and the corresponding percentage of unemployment
  (`unemployment_rate`).

## Problem 2

This problem uses the *Mr. Trash Wheel* dataset.

Read and clean the *Mr. Trash Wheel* sheet:

- specify the sheet in the Excel file and to omit non-data entries (rows
  with notes / figures; columns containing notes) using arguments in
  `read_excel`

- use reasonable variable names

- omit rows that do not include dumpster-specific data

``` r
mr_df =
  read_excel(
    "data/Trash Wheel Collection Data.xlsx",
    sheet = 1,
    range = "A2:N586"
  ) |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year)
  )
```

The data include a column for the (approximate) number of homes powered.
This calculation is described in the `Homes powered note`, but not
applied to every row in the dataset. Update the data to include a new
`homes_powered` variable based on this calculation.

``` r
mr_df =
  mutate(
    mr_df,
    homes_powered = weight_tons * 500 / 30
  )
```

Use a similar process to import, clean, and organize the data for
*Professor Trash Wheel* and *Gwynnda*, and combine these with the
*Mr. Trash Wheel* dataset to produce a single tidy dataset. To keep
track of which Trash Wheel is which, you may need to add an additional
variable to all datasets before combining.

``` r
prof_df =
  read_excel(
    "data/Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    range = "A2:M108"
  ) |>
  janitor::clean_names() |>
  mutate(
    homes_powered = weight_tons * 500 / 30,
    wheel_name = "prof"
  )

gwyn_df =
  read_excel(
    "data/Trash Wheel Collection Data.xlsx",
    sheet = 4,
    range = "A2:L157"
  ) |>
  janitor::clean_names() |>
  mutate(
    homes_powered = weight_tons * 500 / 30,
    wheel_name = "gwyn"
  )

mr_df =
  mutate(
    mr_df,
    wheel_name = "mr"
  )
```

``` r
trash_wheel_df =
  bind_rows(mr_df, prof_df, gwyn_df)
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables.

- The `trash_wheel_df` combines 3 datasets, with 584 observations from
  the *Mr. Trash Wheel* dataset, 106 observations from the *Professor
  Trash Wheel* dataset, and 155 observations from the *Gwynnda Trash
  Wheel* dataset.

- Each observation contains the date (`month`, `year` and `date`) and
  the corresponding trash collecting records of the trash wheel, such as
  the weight of trash (`weight_tons`), the volume of trash
  (`volume_cubic_yards`) and the number of specific types of trash
  (e.g. `plastic_bottles`, `cigarette_butts`).

For available data, what was the total weight of trash collected by
Professor Trash Wheel?

- The total weight of trash collected by Professor Trash Wheel is 216.26
  (tons).

What was the total number of cigarette butts collected by Gwynnda in
July of 2021?

- The total number of cigarette butts collected by Gwynnda in July of
  2021 is 1.63^{4}.

## Problem 3

This problem uses data collected in an observational study to understand
the trajectory of Alzheimer’s disease (AD) biomarkers.

**Import**, **clean**, and **tidy** the dataset of *baseline
demographics*. Ensure that **sex** and **APOE4 carrier status** are
appropriate encoded (i.e. not numeric), and **remove** any participants
who do not meet the stated inclusion criteria (i.e. no MCI at baseline).

``` r
baseline_df =
  # import data, skip the notes, assign column types
  read_csv("data/data_mci/MCI_baseline.csv", skip = 1, col_types = "iniiin") |> 
  janitor::clean_names() |>
  # only keep the ones with no MCI at baseline
  filter(age_at_onset > current_age | is.na(age_at_onset)) |>
  rename(baseline_age = current_age) |>
  mutate( # properly encode sex and APOE4
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carry",
      0 ~ "none"
    )
  )
```

Discuss important steps in the import process and relevant features of
the dataset.

- Important steps in the import process including skipping the notes in
  the first row, assigning column types, removing participants who
  already had MCI at baseline using `filter()` and encoding `sex` and
  `apoe4` to meaningful values.

- 479 participants (who meet the stated inclusion criteria) were
  recruited, and of these 93 develop MCI.

- The average baseline age is 65.0286013.

- The proportion of APOE4 carriers in the women in the study is 30%.

Similarly, import, clean, and tidy the dataset of *longitudinally
observed biomarker values*; comment on the steps on the import process
and the features of the dataset.

``` r
amyloid_df =
  # import data, skip the notes, assign column types
  read_csv("data/data_mci/mci_amyloid.csv", skip = 1, col_types = "innnnn") |> 
  janitor::clean_names() |>
  rename(id = study_id)
```

Check whether some participants appear in only the baseline or amyloid
datasets, and comment on your findings.

``` r
id_b = pull(baseline_df, id)
id_a = pull(amyloid_df, id)

id_b_only = id_b[!(id_b %in% id_a)] #participants that only appear in baseline
id_a_only = id_a[!(id_a %in% id_b)] #participants that only appear in amyloid
```

- Participants 14, 49, 92, 179, 268, 304, 389, 412 only appear in the
  baseline datasets, and participants 72, 234, 283, 380, 484, 485, 486,
  487, 488, 489, 490, 491, 492, 493, 494, 495 only appear in the amyloid
  datasets.

Combine the demographic and biomarker datasets so that only participants
who appear in both datasets are retained, and briefly describe the
resulting dataset; export the result as a CSV to your data directory.

``` r
ad_df = 
  inner_join(baseline_df, amyloid_df, by = join_by(id))

write_csv(ad_df, "data/data_mci/AD_observations.csv")
# export data to the file "data/data_mci/AD_observations.csv"
```

- The resulting dataset `ad_df` contains the data of 471 participants
  who appear in both datasets, including their basic demographic
  information (`id`, `baseline_age`, `sex`, `education` (years) ), APOE4
  carrier status (`apoe4`), `age_at_onset` of MCI and amyloid β 42/40
  ratio over time (`baseline`, `time_2`, `time_`4, `time_6`, `time_8`).

- The results has been exported as `AD_observations.csv` to the data
  directory `data/data_mci/`.

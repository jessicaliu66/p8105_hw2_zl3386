---
title: "p8105_hw2_zl3386"
author: "Ziqiu Liu"
date: "2023-09-26"
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(readxl)
```


## Problem 1

This problem uses the _FiveThirtyEight_ data. 

In particular, we’ll use the data in `pols-month.csv`, `unemployment.csv`, and `snp.csv`. 

Our goal is to merge these into a single data frame using `year` and `month` as keys across datasets.

__First__, clean the data in `pols-month.csv`. 

* Use `separate()` to break up the variable `mon` into integer variables `year`, `month`, and `day`
* Replace month number with month name
* Create a `president` variable taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`
* Remove the `day` variable.

```{r, message = FALSE}
pols_df = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "/", convert = TRUE) |>
  mutate(
    president = case_match(
      prez_dem,
      1 ~ "dem",
      2 ~ "dem",
      0 ~ "gop"
    ),
    month = case_match(
      month,
      1 ~ "Jan",
      2 ~ "Feb",
      3 ~ "Mar",
      4 ~ "Apr",
      5 ~ "May",
      6 ~ "Jun",
      7 ~ "Jul",
      8 ~ "Aug",
      9 ~ "Sep",
      10 ~ "Oct",
      11 ~ "Nov",
      12 ~ "Dec"
    )
  ) |>
  select(-starts_with("prez_"), -day)

  
```

__Second__, clean the data in `snp.csv` using a similar process to the above. 

For consistency across datasets, arrange according to `year` and `month`, and organize so that `year` and `month` are the leading columns.

```{r, message = FALSE}
snp_df = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |>
  janitor::clean_names() 

snp_df_1 = 
  snp_df[1:175,] |>
  janitor::clean_names() |>
  separate(date, into = c("year", "month", "day"), sep = "/", convert = TRUE) 

snp_df_2 = 
  snp_df[176:787,] |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day" ,"year"), sep = "/", convert = TRUE) |>
  relocate(year, month) |>
  mutate(
    year = year + 1900,
    year = replace(year, year == 1900, 2000)
  )

snp_df = 
  rbind(snp_df_1, snp_df_2) |>
  mutate(
    month = case_match(
      month,
      1 ~ "Jan",
      2 ~ "Feb",
      3 ~ "Mar",
      4 ~ "Apr",
      5 ~ "May",
      6 ~ "Jun",
      7 ~ "Jul",
      8 ~ "Aug",
      9 ~ "Sep",
      10 ~ "Oct",
      11 ~ "Nov",
      12 ~ "Dec"
    )
  ) |>
  select(-day) |>
  arrange(year, month)

```

__Third__, tidy the `unemployment` data so that it can be merged with the previous datasets. 

This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.

```{r, message = FALSE}
unemployment_df = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |>
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |>
  janitor::clean_names() 
```

Join the datasets by merging `snp` into `pols`, and merging `unemployment` into the result.

```{r}
results_df =
  full_join(snp_df, pols_df, by = c("year", "month")) |>
  full_join(unemployment_df, by = c("year", "month"))
```

Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables).

* Dataset `pols_df` contains the date (`year` and `month`) , the party of the president (`president`) and the number of national politicians who are democratic or republican on the given date.

* Dataset `snp_df` contains 3 columns which indicate the date (`year` and `month`) and the closing values of the S&P stock index on the associated date (`close`).

* Dataset `unemployment_df` contains 3 columns which indicate the date (`year` and `month`) and the corresponding percentage of unemployment (`unemployment_rate`).


## Problem 2

This problem uses the _Mr. Trash Wheel_ dataset.

Read and clean the _Mr. Trash Wheel_ sheet:

* specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in `read_excel`

* use reasonable variable names

* omit rows that do not include dumpster-specific data

```{r mr trash wheel}
mr_df =
  read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx",
    sheet = 1,
    range = "A2:N586"
  ) |>
  janitor::clean_names() |>
  mutate(
    year = as.numeric(year)
  )
```

The data include a column for the (approximate) number of homes powered. This calculation is described in the `Homes powered note`, but not applied to every row in the dataset. Update the data to include a new `homes_powered` variable based on this calculation.

```{r homes_powered}
mr_df =
  mutate(
    mr_df,
    homes_powered = weight_tons * 500 / 30
  )
```

Use a similar process to import, clean, and organize the data for _Professor Trash Wheel_ and _Gwynnda_, and combine these with the _Mr. Trash Wheel_ dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to all datasets before combining.

```{r professor and Gwynnda trash wheel, message = FALSE}
prof_df =
  read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    range = "A2:M108"
  ) |>
  janitor::clean_names() |>
  mutate(
    homes_powered = weight_tons * 500 / 30,
    wheel_name = "prof"
  )

gwyn_df =
  read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx",
    sheet = 4,
    range = "A2:L157"
  ) |>
  janitor::clean_names() |>
  mutate(
    homes_powered = weight_tons * 500 / 30,
    wheel_name = "gwyn"
  )

mr_df =
  mutate(
    mr_df,
    wheel_name = "mr"
  )
```

```{r combine 3 datasets}
trash_wheel_df =
  bind_rows(mr_df, prof_df, gwyn_df)
```

Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. 

* The `trash_wheel_df` combines 3 datasets, with `r nrow(filter(trash_wheel_df, wheel_name == "mr"))` observations from the _Mr. Trash Wheel_ dataset,  `r nrow(filter(trash_wheel_df, wheel_name == "prof"))` observations from the _Professor Trash Wheel_ dataset, and `r nrow(filter(trash_wheel_df, wheel_name == "gwyn"))` observations from the _Gwynnda Trash Wheel_ dataset.

* Each observation contains the date (`month`, `year` and `date`) and the corresponding trash collecting records of the trash wheel, such as the weight of trash (`weight_tons`), the volume of trash (`volume_cubic_yards`) and the number of specific types of trash (e.g. `plastic_bottles`, `cigarette_butts`).

For available data, what was the total weight of trash collected by Professor Trash Wheel? 

* The total weight of trash collected by Professor Trash Wheel is `r sum(prof_df[,"weight_tons"])` (tons).

What was the total number of cigarette butts collected by Gwynnda in July of 2021?

* The total number of cigarette butts collected by Gwynnda in July of 2021 is `r sum(filter(gwyn_df, year == 2021, month == "July")[,"cigarette_butts"])`.


## Problem 3

This problem uses data collected in an observational study to understand the trajectory of Alzheimer’s disease (AD) biomarkers. 

__Import__, __clean__, and __tidy__ the dataset of _baseline demographics_. Ensure that __sex__ and __APOE4 carrier status__ are appropriate encoded (i.e. not numeric), and __remove__ any participants who do not meet the stated inclusion criteria (i.e. no MCI at baseline). 

```{r baseline dataset, warning = FALSE}
baseline_df =
  # import data, skip the notes, assign column types
  read_csv("data/data_mci/MCI_baseline.csv", skip = 1, col_types = "iniiin") |> 
  janitor::clean_names() |>
  # only keep the ones with no MCI at baseline
  filter(age_at_onset > current_age | is.na(age_at_onset)) |>
  rename(baseline_age = current_age) |>
  mutate( # properly encode sex and APOE4
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carry",
      0 ~ "none"
    )
  )
  
```

Discuss important steps in the import process and relevant features of the dataset.

* Important steps in the import process including skipping the notes in the first row, assigning column types, removing participants who already had MCI at baseline using `filter()` and encoding `sex` and `apoe4` to meaningful values.

* `r nrow(baseline_df)` participants (who meet the stated inclusion criteria) were recruited, and of these `r nrow(filter(baseline_df, age_at_onset > 0))` develop MCI.

* The average baseline age is `r mean(pull(baseline_df, baseline_age))`.

* The proportion of APOE4 carriers in the women in the study is `r 100 * nrow(filter(baseline_df, sex == "female", apoe4 == "carry")) / nrow(filter(baseline_df, sex == "female"))`%.

Similarly, import, clean, and tidy the dataset of _longitudinally observed biomarker values_; comment on the steps on the import process and the features of the dataset.

```{r biomarker dataset, warning = FALSE}
amyloid_df =
  # import data, skip the notes, assign column types
  read_csv("data/data_mci/mci_amyloid.csv", skip = 1, col_types = "innnnn") |> 
  janitor::clean_names() |>
  rename(id = study_id) |>
  pivot_longer(
    baseline:time_8,
    names_to = "years_from_baseline",
    values_to = "amyloid_ratio"
  ) |>
  mutate(
    years_from_baseline = case_match(
      years_from_baseline,
      "baseline" ~ 0,
      "time_2" ~ 2,
      "time_4" ~ 4,
      "time_6" ~ 6,
      "time_8" ~ 8
    )
  )
```

Check whether some participants appear in only the baseline or amyloid datasets, and comment on your findings.

```{r check unique participants}
id_b = unique(pull(baseline_df, id))
id_a = unique(pull(amyloid_df, id))

id_b_only = id_b[!(id_b %in% id_a)] #participants that only appear in baseline
id_a_only = id_a[!(id_a %in% id_b)] #participants that only appear in amyloid
```

* Participants `r id_b_only` only appear in the baseline datasets, and participants `r id_a_only` only appear in the amyloid datasets.

Combine the demographic and biomarker datasets so that only participants who appear in both datasets are retained, and briefly describe the resulting dataset; export the result as a CSV to your data directory.

```{r combine and export}
ad_df = 
  inner_join(baseline_df, amyloid_df, by = join_by(id))

write_csv(ad_df, "data/data_mci/AD_observations.csv")
# export data to the file "data/data_mci/AD_observations.csv"
```

* The resulting dataset `ad_df` contains the data of `r ad_df |> pull(id) |> unique() |> length()` participants who appear in both datasets, including their basic demographic information (`id`,  `baseline_age`, `sex`, `education` (years) ), APOE4 carrier status (`apoe4`), `age_at_onset` of MCI and their amyloid β 42/40 ratio (`amyloid_ratio`) at the time of measurement (`years_from_baseline`).

* `r ad_df |> filter(apoe4 == "carry") |> pull(id) |> unique() |> length()` of the participants are APOE4 carriers, and `r ad_df |> filter(apoe4 == "carry", age_at_onset > 0) |> pull(id) |> unique() |> length()` of them developed MCI.

* `r ad_df |> filter(apoe4 == "none") |> pull(id) |> unique() |> length()` of the participants are non-carriers, and `r ad_df |> filter(apoe4 == "none", age_at_onset > 0) |> pull(id) |> unique() |> length()` of them developed MCI.

* The results has been exported as `AD_observations.csv` to the data directory `data/data_mci/`.